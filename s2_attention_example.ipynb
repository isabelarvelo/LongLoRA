{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYwfxIVqexWgh0108VWFSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabelarvelo/LongLoRA/blob/main/s2_attention_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shifted Sparse Attention Code Demo"
      ],
      "metadata": {
        "id": "Uab43dQGnHFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "LuTF7bX9fZqe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B: batch size; S: sequence length or number of tokens; G: group size;\n",
        "# H: number of attention heads; D: dimension of each attention head"
      ],
      "metadata": {
        "id": "bVZv3rDXgInP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 8\n",
        "num_heads = 4\n",
        "head_dim = 16\n",
        "group_size = 4"
      ],
      "metadata": {
        "id": "NfvH4ThDfisT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(batch_size, seq_len, num_heads, head_dim)\n",
        "assert seq_len % group_size == 0, \"Sequence length must be divisible by group size\""
      ],
      "metadata": {
        "id": "8uLLFA4nfosm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split heads into two groups\n",
        "x1, x2 = x.chunk(2, dim=2)"
      ],
      "metadata": {
        "id": "Dne0P1azfvby"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1.shape, x2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETyCE0C_fwxC",
        "outputId": "9bc74db7-931f-427d-852f-6d4dd97d5b18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 2, 16]), torch.Size([1, 8, 2, 16]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "B, N, H, D = 1, 16, 4, 8  # Batch size, Sequence length, Number of heads, Head dimension\n",
        "G = 4  # Group size\n",
        "\n",
        "# Create dummy input tensors\n",
        "query_states = torch.randn(B, N, H, D)\n",
        "key_states = torch.randn(B, N, H, D)\n",
        "value_states = torch.randn(B, N, H, D)"
      ],
      "metadata": {
        "id": "B4vGQzpyjF6m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_states.shape, key_states.shape, value_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmeOBgPyjQc2",
        "outputId": "d3659f7e-2dda-453d-8ea4-a4c7e763a9ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 16, 4, 8]),\n",
              " torch.Size([1, 16, 4, 8]),\n",
              " torch.Size([1, 16, 4, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `shift` function is the core of the S2-Attn mechanism. It does two main things:\n",
        "\n",
        "1. Shifting: It takes the second half of the attention heads and shifts them by half the group size. This allows for information to flow between different parts of the sequence.\n",
        "2. Reshaping: It reorganizes the data into a shape that's suitable for grouped attention. This step divides the sequence into groups and prepares them for parallel processing.\n",
        "\n",
        "After defining this function, it's applied to the query, key, and value states. This prepares all three components for the S2-Attn mechanism, ensuring they're all shifted and reshaped in the same way."
      ],
      "metadata": {
        "id": "07UAvy2Aj-fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shift(qkv, bsz, q_len, group_size, num_heads, head_dim):\n",
        "    # Shift the second half of the heads by half the group size\n",
        "    # This creates an offset that allows for information flow between groups\n",
        "    qkv[:, num_heads // 2:] = qkv[:, num_heads // 2:].roll(-group_size // 2, dims=1)\n",
        "\n",
        "    # Reshape the tensor for grouped attention:\n",
        "    # 1. Transpose to move heads dimension before sequence dimension\n",
        "    # 2. Reshape to group the sequence\n",
        "    # 3. Transpose again to put group dimension before heads\n",
        "    qkv = qkv.transpose(1, 2).reshape(bsz * (q_len // group_size), group_size, num_heads, head_dim).transpose(1, 2)\n",
        "    return qkv\n",
        "\n",
        "# Apply the shift operation to query, key, and value states\n",
        "# This prepares all three components for the S2-Attn mechanism\n",
        "query_states = shift(query_states, B, N, G, H, D)\n",
        "key_states = shift(key_states, B, N, G, H, D)\n",
        "value_states = shift(value_states, B, N, G, H, D)"
      ],
      "metadata": {
        "id": "_Ta4cCDXjPxe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_states.shape, key_states.shape, value_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOEi2ZAfjYwd",
        "outputId": "8d6a98c3-4f34-47ed-8a29-22b1c84fa82f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 4, 4, 8]), torch.Size([4, 4, 4, 8]), torch.Size([4, 4, 4, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see the batch size is multiplied by a factor of 4 and for each batch, the sequence length is 1/4 the original."
      ],
      "metadata": {
        "id": "5PPelPxWkZDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate attention (in reality, you would perform actual attention here)\n",
        "# For demonstration, we'll just return the query states\n",
        "attn_output = query_states"
      ],
      "metadata": {
        "id": "u5uST0wqkK-c"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape back to original shape\n",
        "attn_output = attn_output.transpose(1, 2).reshape(B, N, H, D)"
      ],
      "metadata": {
        "id": "Ye74r_TbkvsQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break it down:\n",
        "\n",
        "`attn_output.transpose(1, 2)`:\n",
        "\n",
        "* This swaps the second and third dimensions.\n",
        "* The shape changes from (B * (N // G), H, G, D) to (B * (N // G), G, H, D).\n",
        "\n",
        "\n",
        "`.reshape(B, N, H, D)`\n",
        "\n",
        "* This reshapes the tensor back to its original dimensions.\n",
        "* It combines the first two dimensions (B * (N // G) and G) back into B and N.\n",
        "\n",
        "The reason this works is that the total number of elements hasn't changed; we're just reorganizing them. The grouped structure we created earlier (B * (N // G), G) is now being \"unfolded\" back into (B, N). This reshaping operation is essentially reversing the grouping we did earlier in the shift function. It's taking our grouped, shifted attention output and reorganizing it back into the original sequence order and shape, which is necessary for further processing or for the output of the layer."
      ],
      "metadata": {
        "id": "HqqDp_AilAC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2doaGHWk0vD",
        "outputId": "98d83ad2-dbcc-4ed4-efd0-85dfcd66bc5a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unshift the second half of heads\n",
        "attn_output[:, :, H // 2:] = attn_output[:, :, H // 2:].roll(G // 2, dims=1)"
      ],
      "metadata": {
        "id": "jkFpixfImTt0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*   `attn_output[:, :, H // 2:]`: selects the second half of the attention heads\n",
        "*   `.roll(G // 2, dims=1)`: rolls (circularly shifts) these heads along the sequence dimension (dim=1) by half the group size (G // 2).\n",
        "This rolling operation is the inverse of the initial shift we applied in the shift function."
      ],
      "metadata": {
        "id": "36RT8yI1mdVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8IyspLtmXHR",
        "outputId": "ed29e163-668e-407e-b8e6-f3760a7623f7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA3JTlmWivka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}